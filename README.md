# LLMs from Scratch

Hi!

In this repository, I'm building **Large Language Models** from *scratch* using nothing but PyTorch. The goal is to fundamentally understand popular LLM architectures that make Generative AI possible. Starting with core concepts like Attention (self, multi-head, cross), Tokenization and Embedding Models, I'm studying the most popular Language Models that are essentially milestones in the Gen AI landscape.

1. Transformers (Google) 
2. GPT 1 (OpenAI)
3. BERT (Google)
4. GPT 2 (OpenAI)
5. T5 (Google)
6. InstructGPT (OpenAI) (More RL, but okay)
7. LLama Family

Another track is to explore types of model architectures
1. Encoder-only vs Decoder-only
2. Knowledge Distillation (DistilBERT)
3. Mixture of Experts (MoE)

Finally, moving from a fundamental, mathematical approach, I will move towards optimization and hardware technologies.
1. Fine Tuning (SFT, PEFT, LoRA)
2. Quantization
3. Distributed training
4. RL Optimizations (RLHF, DPO, PPO)